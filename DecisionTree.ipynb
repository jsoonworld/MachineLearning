{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Decision Tree\n",
        "\n",
        "* 결정트리는 특정 기준에 의해 훈련 데이터를 '나무' 구조로 나눠가는 모델\n",
        "* 결정트리는 예측 결과의 인과성을 설명하기 적합함. 또한 열정적 모델로 학습 시 미리 학습된 계수를 이용해 예측이 빠름, 반면 KNN은 테스트 데이터를 받은 후 학습 데이터를 이용해서 이웃을 구함(예측이 느림, 게으른 모델)\n",
        "* 결정트리 (=의사결정나무)는 동일한 데이터를 어떤 규칙과 순서로 나누는지에 따라 결과가 달라짐.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4yC8_PJjFqWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##결정트리를 나누는 특정 기준이란?\n",
        "\n",
        "* 결정트리는 학습 데이터를 이용해 데이터를 최적으로 분류해주는 질문들을 학습하는 머신러닝\n",
        "* 불순도 : 데이터가 분류되지 않고 섞여 있는 정도\n",
        "* 분순도가 낮음: 분류가 명확하게 나뉨\n",
        "\n"
      ],
      "metadata": {
        "id": "5bM5aPeKVdw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##지니 계수\n",
        "* 경제적 불평등을 표현하는 방법으로 0에 가까울 수록 평등함.\n",
        "* 결정 트리는 지니 계수를 이용해 노드를 나눔.\n"
      ],
      "metadata": {
        "id": "GFLuAwpnVqE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##정보이득: 부모와 지식 노드 사이의 불순도 차이\n",
        "* 결정트리는 정보 이득이 최대가 되도록 데이터를 나눔\n",
        "- 자식노드의 데이터 불순도가 작으면 작을 수록 커지게 됨.\n",
        "\n"
      ],
      "metadata": {
        "id": "_FdTfhHaV2tu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##가지치기\n",
        "* 노드 깊이를 제한하여 결정트리가 Overfit 되는 것을 방지\n",
        "* 학습 데이터를 기준으로 매우 깊은(분류를 위한 모든 경우의 수를 고려)트리를 만들 경우 학습데이터에만 최적화된 학습 결과가 나올 수 있음. 이를 방지하기 위해 노드의 깊이를 제한\n",
        "\n"
      ],
      "metadata": {
        "id": "cD9EtS26WC-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##표준화 하지 않은 데이터로 학습하기\n",
        "* 결정트리는 Value값 자체만으로 예측하기 떄문에, 입력데이터를 표준화 하지 않아도 됨.\n",
        "\n"
      ],
      "metadata": {
        "id": "0kqBq-mlWPcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 결정 트리의 depth를 변경하여 최적의 세팅 찾기\n",
        "* Test data에 가장 성능이 높은 depth값을 직접 변경해보고 가장 좋은 성능을 보이는 값을 찾자!\n"
      ],
      "metadata": {
        "id": "1MPypGdYWYyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##검증 데이터(Validation Data)\n",
        "* 데이터를 학습/테스트에서 학습/검증/테스트로 3분할하여 테스트 데이터의 독립성을 유지\n",
        "- 검증 데이터는 최적의 depth를 찾는데 활용됨.\n"
      ],
      "metadata": {
        "id": "xj73EEFuX9Hi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##k-fold cross validation\n",
        "\n",
        "- 학습/검증/테스트로 3분할하면 학습데이터 양이 작아져서 성능이 낮아짐, 이를 해결하기 위해 활용\n",
        "- 학습데이터에서 소량의 검증 세트를 떼어내어 모델을 여러개 만들고 평가하는 과정을 반복\n"
      ],
      "metadata": {
        "id": "gdXmT_nPYLhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##cross validation\n",
        "\n",
        "- 일반적인 교차검증 k=5: 회귀모델은 kFold분할기 사용\n",
        "- 분류모델은 stratifiedKFold 사용\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vKekp1KBYh5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Grid Search와 Hyperparameter\n",
        "\n",
        "- Hyper parameter: 사용자 지정 파라미터\n",
        "- model parameter: 모델이 학습하는 파라미터 값\n",
        "- Hyper parameter tuning: Hyperparameter 값을 변경하며 최적의 값을 찾는 것.\n"
      ],
      "metadata": {
        "id": "8MyZerU1Yyfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyper parameter tuning의 난점\n",
        "- 최적의 max_depth를 수작업으로 찾는다 해도 다른 하이퍼파라미터 min_sample_split의 설정 값에 따라 max_depth의 최적값이 변화됨.\n",
        "\n",
        "- 하이퍼파라미터 튜닝의 해결: for 반복문 해결 or Grid Search, random search\n",
        "\n"
      ],
      "metadata": {
        "id": "v46CoJkaZJtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Grid Search & Hyperparameter\n",
        "\n",
        "- 그리드서치(격자탐색): 특정 간격에서 모델에게 가장 적합한 하이퍼파라미터를 찾기\n",
        "\n"
      ],
      "metadata": {
        "id": "GL1rLV65Z8Wd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Raddom Search와 확률 분포\n",
        "\n",
        "## 그리드서치(격자탐색)의 문제점\n",
        "- 하이퍼파라미터가 추가될 때마다 학습해야 할 모델 수가 n승으로 상승\n",
        "- 사람이 개입해서 하이퍼파라미터의 값의 범위를 직접 지정해 줘야함.\n",
        "\n",
        "## 그리드서치의 해결을 위해 랜덤서치 활용가능.\n",
        "\n"
      ],
      "metadata": {
        "id": "nCaibH6yaaF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Search와 확률 분포\n",
        "\n",
        "- 랜덤서치: 그리드서치처럼 하이퍼파라미터 값을 직접 전달하지 않고 확률 분포를 전달해 그중 랜덤으로 샘플링\n",
        "\n"
      ],
      "metadata": {
        "id": "BtY6vIZZatgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scipy를 이용해 확률분포 생성."
      ],
      "metadata": {
        "id": "tdi82nDwa4kn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ensemble\n",
        "- 모델 하나를 학습하기보다 다수의 모델을 학습하여 최종 의사결정(예측)을 진행하는 모델\n",
        "\n",
        "- Decision tree에 ensemble을 적용하면? -> randomforest"
      ],
      "metadata": {
        "id": "uOC5E-b2dEz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Structured data or Unstructured data"
      ],
      "metadata": {
        "id": "d3TwPpQQdH84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eensemble의 분류\n",
        "\n",
        "- Bagging: 같은 알고리즘을 사용하여 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습\n",
        "\n",
        "- Boosting: 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법.\n",
        "\n"
      ],
      "metadata": {
        "id": "zqfP_uGFdfuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bagging\n",
        "\n",
        "- Random Forest: 결정트리를 랜덤하게 여러개 만들어 결정트리의 숲을 만드는 방법\n",
        "\n",
        "- 결정트리를 랜덤하게 만드는 방법: 부스트랩 샘플\n",
        "- (랜덤 데이터) 학습데이터로부터 중복을 허용하여 랜덤하게 데이터를 샘플링하는 방법. \n",
        "\n",
        "- (랜덤 특성)노드를 분할 할때 특성 중 일부 특성을 무작위로 고르는 무작위성을 더함\n",
        "\n"
      ],
      "metadata": {
        "id": "0EOBke2-dtQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Forest\n",
        "- 결정트리를 랜덤하게 여러개 만들어 결정트리의 숲을 만드는 방법\n",
        "- 기본적으로 100개의 결정트리를 학습해서 하나의 랜덤포레스트를 만듬.\n"
      ],
      "metadata": {
        "id": "ALnrD27YeOil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extra Trees\n",
        "\n",
        "- 정의: 부스트랩 샘플을 사용하지 않고 결정트리의 특성을 랜덤하게, 노드 분할 기준도 랜덤하게 만든 모델\n",
        "\n",
        "- 장점: 모든 학습데이터가 저혀 누락되지 않고 사용됨, 특성을 무작위로 선택하기 때문에 과대적합을 막음 랜덤하게 노드를 분할하여 속도가 빠름 (기존 결정트리는 노드 분할 기준이 \"best\"이걸 랜덤으로)\n",
        "\n",
        "- 단점: 랜덤하게 노드를 분할하여 무작위성이 크기 때문에 더 많은 결정트리를 훈련 해야함.\n",
        "\n"
      ],
      "metadata": {
        "id": "OxlA07qyeaGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Boosting 기반의 앙상블 방법\n",
        "\n",
        "- Boosting: 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법\n",
        "- 에이다부스트: 이전 예측기를 보완하는 새로운 예측기를 만드는 방법"
      ],
      "metadata": {
        "id": "V2wiJKxrfPNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient Boosting\n",
        "- 정의: 에이다부스트처럼 반복 마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차에 새로운 예측기를 학습\n",
        "\n"
      ],
      "metadata": {
        "id": "ymESk4GQfcq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##다양한 부스팅 기반의 앙상블 방법\n",
        "- 히스토그램 기반 그래디언트 부스팅 / XGBoost / LightGBM"
      ],
      "metadata": {
        "id": "ouq2ysa7fn0N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-yt3u1bD_M_"
      },
      "outputs": [],
      "source": []
    }
  ]
}